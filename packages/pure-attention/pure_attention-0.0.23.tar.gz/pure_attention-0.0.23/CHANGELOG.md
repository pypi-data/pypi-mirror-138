## 0.0.23 (2022-02-6)


### Bug Fixes

* **bert:** change LayerNorm to layer_norm ([a99831e](https://github.com/mmmwhy/pure_attention/commit/a99831ee3b4ad06cadbb0262720c0836717d7508))
* **type:** fix some code mistake ([3e1a81d](https://github.com/mmmwhy/pure_attention/commit/3e1a81dd351f2a31ca03fce7cf8ca80be2b94a6d))


### Features

* **bert:** add tokenizer part ([054df14](https://github.com/mmmwhy/pure_attention/commit/054df14c7dfefc0b2edb47824578b33f4a5c8539))
* **decode:** add some transformer decode code ([893be87](https://github.com/mmmwhy/pure_attention/commit/893be87901aa875488a5bdce53bcf11f1bf74033))
* **layers:** fix import for layerNorm ([eb61b31](https://github.com/mmmwhy/pure_attention/commit/eb61b313458ac18bf4b15271fee2cf7e39f8afde))
* **nlp:** init basic bert code ([f9cb13a](https://github.com/mmmwhy/pure_attention/commit/f9cb13a3e811eb8c44ba8ff1373d688311426927))



