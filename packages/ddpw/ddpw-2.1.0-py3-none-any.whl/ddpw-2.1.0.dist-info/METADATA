Metadata-Version: 2.1
Name: ddpw
Version: 2.1.0
Summary: A utility package to scaffold PyTorch's DDP
Home-page: http://ddpw.projects-tvs.surge.sh
Author: Sujal T.V.
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: MacOS
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: POSIX :: Linux
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: numpy (==1.22.2)
Requires-Dist: pip (==22.0.3)
Requires-Dist: setuptools (==60.7.1)
Requires-Dist: submitit (==1.2.1)
Requires-Dist: torch (==1.10.1)
Requires-Dist: wheel (==0.37.1)

# DDPW

[![Surge](https://img.shields.io/badge/documentation-surge-blue?link=http://ddpw.projects-tvs.surge.sh)](http://ddpw.projects-tvs.surge.sh)
[![Conda](https://img.shields.io/conda/v/tvsujal/ddpw)](https://anaconda.org/tvsujal/ddpw)
[![PyPI](https://img.shields.io/pypi/v/ddpw)](https://pypi.org/project/ddpw/)

[![Publish documentation to Surge](https://github.com/sujaltv/ddpw/actions/workflows/surge_publish.yaml/badge.svg)](https://github.com/sujaltv/ddpw/actions/workflows/surge_publish.yaml)
[![Publish to Anaconda](https://github.com/sujaltv/ddpw/actions/workflows/conda_publish.yaml/badge.svg)](https://github.com/sujaltv/ddpw/actions/workflows/conda_publish.yaml)
[![Publish to PyPI](https://github.com/sujaltv/ddpw/actions/workflows/pypi_publish.yaml/badge.svg)](https://github.com/sujaltv/ddpw/actions/workflows/pypi_publish.yaml)

---

The Distributed Data Parallel wrapper (DDPW) is created as a utility package to
encapsulate the scaffolding for PyTorch's Distributed Data Parallel.

This code is written in Python 3.8. The [DDPW
documentation](http://ddpw.projects-tvs.surge.sh) contains details on how to use
this package.

## Overview

### Installation

```bash
conda install -c tvsujal ddpw # with conda
pip install ddpw # with pip from PyPI
```

### Usage

```python
  from ddpw.platform import PlatformConfig
  from ddpw.artefacts import ArtefactsConfig
  from ddpw.trainer import TrainingConfig
  from ddpw.wrapper import Wrapper

  from src import CustomTrainer

  p = PlatformConfig(...)
  a = ArtefactsConfig(...)
  t = TrainingConfig(...)

  d = Wrapper(p, a)
  j = CustomTrainer(t)

  d.start(j)
```


