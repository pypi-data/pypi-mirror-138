{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetCDF Zarr Sequential Recipe: CMIP6\n",
    "\n",
    "This tutorial describes how to create a suitable recipe for many of the CMIP6 datasets.\n",
    "The source data is a sequence of NetCDF files accessed from the 's3://esgf-world' bucket.\n",
    "The target is a Zarr store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "- The s3://esgf-world bucket has about 250,000 datasets stored in about 950,000 netcdf files (for an average of about four netcdf files per dataset). This is a small subset of the WCRP-CMIP6 collection available at the Federated ESGF-COG nodes such as https://esgf-node.llnl.gov/search/cmip6, but it is faster and easier to work with. \n",
    "\n",
    "- Each CMIP6 dataset can be identified by a 6-tuple consisting of:\n",
    "\n",
    "        (model,experiment,ensemble_member,mip_table,variable,grid_label)\n",
    "        \n",
    "and so a convenient name for a particular dataset is a string of these values joined with a '.' separator:\n",
    "\n",
    "      dataset = model.experiment.ensemble_member.mip_table.variable.grid_label\n",
    "        \n",
    "\n",
    "- There can be multiple versions of a dataset, designated by a string beginning with 'v' and then an 8 digit date, loosely associated with its creation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get to know your source data\n",
    "The CMIP6 collection is very heterogeneous, so getting to know the source data is rather complicated. We first need to identify a dataset and learn how to list the set of netcdf files which are associated with it. Fortunately, you can explore the data here: https://esgf-world.s3.amazonaws.com/index.html#CMIP6/ or download a CSV file listing all of the netcdf files, one per line.\n",
    "\n",
    "Here we will read the CSV file into a pandas dataframe so we can search, sort and subset the available datasets and their netcdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1151019 entries, 0 to 1151018\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count    Dtype \n",
      "---  ------           --------------    ----- \n",
      " 0   project          1151019 non-null  object\n",
      " 1   institution_id   1151019 non-null  object\n",
      " 2   source_id        1151019 non-null  object\n",
      " 3   experiment_id    1151019 non-null  object\n",
      " 4   frequency        613552 non-null   object\n",
      " 5   modeling_realm   613552 non-null   object\n",
      " 6   table_id         1151019 non-null  object\n",
      " 7   member_id        1151019 non-null  object\n",
      " 8   grid_label       1151019 non-null  object\n",
      " 9   variable_id      1151019 non-null  object\n",
      " 10  temporal_subset  1122305 non-null  object\n",
      " 11  version          1151019 non-null  object\n",
      " 12  path             1151019 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 114.2+ MB\n"
     ]
    }
   ],
   "source": [
    "netcdf_cat = 's3://cmip6-nc/esgf-world.csv.gz'\n",
    "df_s3 = pd.read_csv(netcdf_cat, dtype='unicode')\n",
    "df_s3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>modeling_realm</th>\n",
       "      <th>table_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>temporal_subset</th>\n",
       "      <th>version</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>AS-RCEC</td>\n",
       "      <td>TaiESM1</td>\n",
       "      <td>histSST-piNTCF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AERmon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gn</td>\n",
       "      <td>ps</td>\n",
       "      <td>185001-201412</td>\n",
       "      <td>v20200318</td>\n",
       "      <td>s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>AS-RCEC</td>\n",
       "      <td>TaiESM1</td>\n",
       "      <td>histSST-piNTCF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CFmon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gn</td>\n",
       "      <td>ta</td>\n",
       "      <td>185001-201412</td>\n",
       "      <td>v20200318</td>\n",
       "      <td>s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>AS-RCEC</td>\n",
       "      <td>TaiESM1</td>\n",
       "      <td>histSST-piNTCF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LImon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gn</td>\n",
       "      <td>snc</td>\n",
       "      <td>185002-201412</td>\n",
       "      <td>v20200318</td>\n",
       "      <td>s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>AS-RCEC</td>\n",
       "      <td>TaiESM1</td>\n",
       "      <td>histSST-piNTCF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LImon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gn</td>\n",
       "      <td>snd</td>\n",
       "      <td>185002-201412</td>\n",
       "      <td>v20200318</td>\n",
       "      <td>s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>AS-RCEC</td>\n",
       "      <td>TaiESM1</td>\n",
       "      <td>histSST-piNTCF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LImon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gn</td>\n",
       "      <td>snw</td>\n",
       "      <td>185002-201412</td>\n",
       "      <td>v20200318</td>\n",
       "      <td>s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  project institution_id source_id   experiment_id frequency modeling_realm  \\\n",
       "0   CMIP6        AS-RCEC   TaiESM1  histSST-piNTCF       NaN            NaN   \n",
       "1   CMIP6        AS-RCEC   TaiESM1  histSST-piNTCF       NaN            NaN   \n",
       "2   CMIP6        AS-RCEC   TaiESM1  histSST-piNTCF       NaN            NaN   \n",
       "3   CMIP6        AS-RCEC   TaiESM1  histSST-piNTCF       NaN            NaN   \n",
       "4   CMIP6        AS-RCEC   TaiESM1  histSST-piNTCF       NaN            NaN   \n",
       "\n",
       "  table_id member_id grid_label variable_id temporal_subset    version  \\\n",
       "0   AERmon  r1i1p1f1         gn          ps   185001-201412  v20200318   \n",
       "1    CFmon  r1i1p1f1         gn          ta   185001-201412  v20200318   \n",
       "2    LImon  r1i1p1f1         gn         snc   185002-201412  v20200318   \n",
       "3    LImon  r1i1p1f1         gn         snd   185002-201412  v20200318   \n",
       "4    LImon  r1i1p1f1         gn         snw   185002-201412  v20200318   \n",
       "\n",
       "                                                path  \n",
       "0  s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...  \n",
       "1  s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...  \n",
       "2  s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...  \n",
       "3  s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...  \n",
       "4  s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiES...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So there are 956,306 entries, one for each netcdf file. We can see the first five here:\n",
    "# The 'path' column is the most important - you may need to scroll the window to see it!\n",
    "\n",
    "df_s3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will add a new column which is our short name for the datasets (may take a moment for all 956306 rows)\n",
    "df_s3['dataset'] = df_s3.apply(lambda row: '.'.join(row.path.split('/')[6:12]),axis=1)\n",
    "# the number of unique dataset names can be found using the 'nunique' method\n",
    "df_s3.dataset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://esgf-world/CMIP6/AerChemMIP/AS-RCEC/TaiESM1/histSST-piNTCF/r1i1p1f1/AERmon/ps/gn/v20200318/ps_AERmon_TaiESM1_histSST-piNTCF_r1i1p1f1_gn_185001-201412.nc'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The value in the `path` column of the first row is:\n",
    "df_s3.path.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TaiESM1.histSST-piNTCF.r1i1p1f1.AERmon.ps.gn'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which has the short name:\n",
    "df_s3.dataset.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL-ESM4.1pctCO2-bgc.r1i1p1f1.Amon.tasmin.gr1 ['v20180701' 'v20190920']\n",
      "BCC-CSM2-MR.historical.r3i1p1f1.Amon.tauv.gn ['v20181119' 'v20190403']\n",
      "CanESM5.historical.r23i1p1f1.Omon.fgo2.gn ['v20190306' 'v20190429']\n",
      "E3SM-1-0.historical.r1i1p1f1.Amon.rlds.gr ['v20190730' 'v20190913']\n",
      "IPSL-CM6A-LR.piControl.r1i1p1f1.Ofx.basin.gn ['v20180802' 'v20181022' 'v20181123']\n",
      "MIROC6.abrupt-4xCO2.r1i1p1f1.Amon.rsuscs.gn ['v20190311' 'v20190705']\n",
      "CESM2-WACCM.amip.r3i1p1f1.Emon.u2.gn ['v20190220' 'v20190319']\n",
      "NorCPM1.historical.r21i1p1f1.Amon.rtmt.gn ['v20190914' 'v20200724']\n",
      "NorESM2-LM.piControl.r1i1p1f1.Omon.zo2min.gr ['v20191108' 'v20210118']\n",
      "CanESM5.hist-CO2.r2i1p1f1.Omon.hfds.gn ['v20190306' 'v20190429']\n",
      "CanESM5.hist-nat.r10i1p1f1.Omon.wo.gn ['v20190306' 'v20190429']\n",
      "CanESM5.ssp245-GHG.r1i1p1f1.Lmon.cLeaf.gn ['v20190306' 'v20190429']\n",
      "CanESM5.ssp245-stratO3.r2i1p1f1.Omon.tauvo.gn ['v20190306' 'v20190429']\n",
      "NorESM2-LM.hist-GHG.r1i1p1f1.AERmon.zg.gn ['v20190815' 'v20191108']\n",
      "MRI-ESM2-0.piClim-4xCO2.r1i1p1f1.LImon.snw.gn ['v20190603' 'v20200114']\n",
      "CanESM5.ssp126.r10i1p1f1.Omon.hfds.gn ['v20190306' 'v20190429']\n",
      "CanESM5.ssp126.r2i1p1f1.Amon.vas.gn ['v20190306' 'v20190429']\n",
      "CanESM5.ssp126.r7i1p1f1.Amon.uas.gn ['v20190306' 'v20190429']\n",
      "CanESM5.ssp585.r1i1p1f1.Amon.ta.gn ['v20190306' 'v20190429']\n",
      "IPSL-CM6A-LR.ssp126.r1i1p1f1.Omon.no3os.gn ['v20190121' 'v20190903']\n",
      "IPSL-CM6A-LR.ssp245.r2i1p1f1.Lmon.lai.gr ['v20190119' 'v20190516']\n",
      "IPSL-CM6A-LR.ssp585.r1i1p1f1.Omon.nh4os.gn ['v20190119' 'v20190903']\n",
      "CESM2-WACCM.ssp585.r1i1p1f1.SImon.siconc.gn ['v20190815' 'v20200702']\n"
     ]
    }
   ],
   "source": [
    "# some datasets have multiple versions: (will just check one in each 500 of them ...)\n",
    "for dataset in df_s3.dataset.unique()[::500]:\n",
    "    df_dataset = df_s3[df_s3.dataset==dataset]\n",
    "    if df_dataset.version.nunique() > 1:\n",
    "        print(dataset,df_dataset.version.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So pick a dataset, any dataset, and try it!  N.B. some datasets are VERY large - especially the day, 6hourly, etc.\n",
    "#dataset = df_s3.dataset[10450]\n",
    "# or:\n",
    "dataset = 'GFDL-CM4.historical.r1i1p1f1.Amon.tas.gr1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>modeling_realm</th>\n",
       "      <th>table_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>temporal_subset</th>\n",
       "      <th>version</th>\n",
       "      <th>path</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629208</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>NOAA-GFDL</td>\n",
       "      <td>GFDL-CM4</td>\n",
       "      <td>historical</td>\n",
       "      <td>mon</td>\n",
       "      <td>atmos</td>\n",
       "      <td>Amon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gr1</td>\n",
       "      <td>tas</td>\n",
       "      <td>185001-194912</td>\n",
       "      <td>v20180701</td>\n",
       "      <td>s3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/...</td>\n",
       "      <td>GFDL-CM4.historical.r1i1p1f1.Amon.tas.gr1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629209</th>\n",
       "      <td>CMIP6</td>\n",
       "      <td>NOAA-GFDL</td>\n",
       "      <td>GFDL-CM4</td>\n",
       "      <td>historical</td>\n",
       "      <td>mon</td>\n",
       "      <td>atmos</td>\n",
       "      <td>Amon</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>gr1</td>\n",
       "      <td>tas</td>\n",
       "      <td>195001-201412</td>\n",
       "      <td>v20180701</td>\n",
       "      <td>s3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/...</td>\n",
       "      <td>GFDL-CM4.historical.r1i1p1f1.Amon.tas.gr1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       project institution_id source_id experiment_id frequency  \\\n",
       "629208   CMIP6      NOAA-GFDL  GFDL-CM4    historical       mon   \n",
       "629209   CMIP6      NOAA-GFDL  GFDL-CM4    historical       mon   \n",
       "\n",
       "       modeling_realm table_id member_id grid_label variable_id  \\\n",
       "629208          atmos     Amon  r1i1p1f1        gr1         tas   \n",
       "629209          atmos     Amon  r1i1p1f1        gr1         tas   \n",
       "\n",
       "       temporal_subset    version  \\\n",
       "629208   185001-194912  v20180701   \n",
       "629209   195001-201412  v20180701   \n",
       "\n",
       "                                                     path  \\\n",
       "629208  s3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/...   \n",
       "629209  s3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/...   \n",
       "\n",
       "                                          dataset  \n",
       "629208  GFDL-CM4.historical.r1i1p1f1.Amon.tas.gr1  \n",
       "629209  GFDL-CM4.historical.r1i1p1f1.Amon.tas.gr1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = df_s3[df_s3.dataset==dataset]\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So is this what we expect?**\n",
    "- this dataset is split over 3 netcdf files - see any trouble here?\n",
    "- lets do a quick sanity check (make sure one and only one variable is specified) and get only the latest version of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable is: tas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/historical/r1i1p1f1/Amon/tas/gr1/v20180701/tas_Amon_GFDL-CM4_historical_r1i1p1f1_gr1_185001-194912.nc',\n",
       " 's3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/historical/r1i1p1f1/Amon/tas/gr1/v20180701/tas_Amon_GFDL-CM4_historical_r1i1p1f1_gr1_195001-201412.nc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvars = df_dataset.variable_id.unique()\n",
    "assert len(dvars) > 0, 'no netcdf files found for this dataset'\n",
    "assert len(dvars) == 1, f\"trouble with this dataset, too many datasets found: {dvars}\"\n",
    "    \n",
    "var = dvars[0]\n",
    "print('The variable is:',var)\n",
    "\n",
    "# make sure we are looking at the last available version:\n",
    "last_version = sorted(df_dataset.version.unique())[-1]\n",
    "dze = df_dataset[df_dataset.version == last_version].reset_index(drop=True)\n",
    "\n",
    "input_urls = sorted(dze.path.unique())\n",
    "input_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are only two files - one netcdf file was from an older version!**\n",
    "- We want to look at the first netcdf file to make sure we know what to expect\n",
    "- To use `xarray.open_dataset`, we need to turn the input_url (starting with 's3://') into an appropriate file_like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (bnds: 2, lat: 180, lon: 288, time: 1200)\n",
      "Coordinates:\n",
      "  * bnds       (bnds) float64 1.0 2.0\n",
      "    height     float64 ...\n",
      "  * lat        (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
      "  * lon        (lon) float64 0.625 1.875 3.125 4.375 ... 355.6 356.9 358.1 359.4\n",
      "  * time       (time) object 1850-01-16 12:00:00 ... 1949-12-16 12:00:00\n",
      "Data variables:\n",
      "    lat_bnds   (lat, bnds) float64 ...\n",
      "    lon_bnds   (lon, bnds) float64 ...\n",
      "    tas        (time, lat, lon) float32 ...\n",
      "    time_bnds  (time, bnds) object ...\n",
      "Attributes: (12/46)\n",
      "    external_variables:     areacella\n",
      "    history:                File was processed by fremetar (GFDL analog of CM...\n",
      "    table_id:               Amon\n",
      "    activity_id:            CMIP\n",
      "    branch_method:          standard\n",
      "    branch_time_in_child:   [0.]\n",
      "    ...                     ...\n",
      "    variable_id:            tas\n",
      "    variant_info:           N/A\n",
      "    references:             see further_info_url attribute\n",
      "    variant_label:          r1i1p1f1\n",
      "    branch_time_in_parent:  [36500.]\n",
      "    parent_time_units:      days since 0001-1-1\n"
     ]
    }
   ],
   "source": [
    "# Connect to AWS S3 storage\n",
    "fs_s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "file_url = fs_s3.open(input_urls[0], mode='rb')\n",
    "ds = xr.open_dataset(file_url)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deciding how to chunk the dataset\n",
    "- For parallel I/O and subsetting the dataset in time, we will chunk the data in the time dimension\n",
    "- In order to figure out the number of time slices in each chunk, we do a small calculation on the first netcdf file\n",
    "- Here we set the desired chunk size to 50 Mb, but something between 50-100 Mb is usually alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bnds': 2, 'lat': 180, 'lon': 288, 'time': 241}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntime = len(ds.time)       # the number of time slices\n",
    "chunksize_optimal = 50e6  # desired chunk size in bytes\n",
    "ncfile_size = ds.nbytes    # the netcdf file size\n",
    "chunksize = max(int(ntime* chunksize_optimal/ ncfile_size),1)\n",
    "\n",
    "target_chunks = ds.dims.mapping\n",
    "target_chunks['time'] = chunksize\n",
    "\n",
    "target_chunks   # a dictionary giving the chunk sizes in each dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define a pre-processing function\n",
    "- This is an optional step which we want to apply to each chunk\n",
    "- Here we change some data variables into coordinate variables, but you can define your own pre-processing step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the netcdf lists some of the coordinate variables as data variables. This is a fix which we want to apply to each chunk.\n",
    "def set_bnds_as_coords(ds):\n",
    "    new_coords_vars = [var for var in ds.data_vars if 'bnds' in var or 'bounds' in var]\n",
    "    ds = ds.set_coords(new_coords_vars)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a recipe\n",
    "- A `FilePattern` is the starting place for all recipes. These Python objects are the \"raw ingredients\" upon which the recipe will act. They describe how the individual source files are organized logically as part of a larger dataset. To create a file pattern, the first step is to define a function which takes any variable components of the source file path as inputs, and returns full file path strings.\n",
    "- Revisting our input urls, we see that the only variable components of these paths are the 13-character numerical strings which immediatly precede the .nc file extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for url in input_urls:\n",
    "    print(f'''{url}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do these strings refer to?**\n",
    "- If it was not immediately apparent, comparison to our dataset coordinates makes it clear that these numerical strings are time ranges; the string `'185001-194912'` from the first url, e.g., represents a time range from Jan 1850 through Dec 1949:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's define a function that takes these strings as input**\n",
    "- ... and returns full file paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_path(time):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : str\n",
    "    \n",
    "        A 13-character string, comprised of two 6-character dates delimited by a dash. \n",
    "        The first four characters of each date are the year, and the final two are the month.\n",
    "        \n",
    "        e.g. The time range from Jan 1850 through Dec 1949 is expressed as '185001-194912'.\n",
    "            \n",
    "    '''\n",
    "    base_url = 's3://esgf-world/CMIP6/CMIP/NOAA-GFDL/GFDL-CM4/historical/r1i1p1f1/Amon/tas/gr1/v20180701/'\n",
    "    return base_url + f'tas_Amon_GFDL-CM4_historical_r1i1p1f1_gr1_{time}.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's be sure to test our function before moving on.\n",
    "\n",
    "test_url = make_full_path('185001-194912')\n",
    "print(test_url)\n",
    "\n",
    "# If our function works, inputting '185001-194912' should have returned a url identical to\n",
    "# the first of the two urls in the list named `input_urls` defined in cell 10, above:\n",
    "\n",
    "test_url == input_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining dimensions**\n",
    "- Before we initialize our file pattern, we need to define how we want files to be combined in our eventual zarr store\n",
    "- We have two options:\n",
    "\n",
    "    1. Concatenating dimensions with a `ConcatDim` instance\n",
    "    2. Merging dimensions with a `MergeDim` instance\n",
    "    \n",
    "    \n",
    "- Our current dataset requires only concatenation, which we can achieve by instantiating `ConcatDim` with our variable name (`\"time\"`) as a positional argument, followed by a `keys` kwarg, which is a list containing all of the ways which this variable appears in our set of source file paths.\n",
    "\n",
    "> **Note:** This example reads from only two source files, so we can simply copy-and-paste their respective time variables into a list. If the number of source files was much larger, we might consider finding a way to create this `keys` list programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.patterns import ConcatDim\n",
    "time_concat_dim = ConcatDim(\"time\", keys=['185001-194912', '195001-201412'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating the file pattern**\n",
    "- Now that we have a both file path function and our \"combine dimensions\" object, we can move on to instantiating to file pattern, passing these two objects as arguments.\n",
    "- Note that we will use `fsspec.open` under the hood for most file opening, so if there are any special keyword arguments we want to pass to this function, now is the time to do it.\n",
    "- Here we specify `fsspec_open_kwargs={'anon':True}` as a keyword argument in the `FilePattern`, because we want to access the source files anonymously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.patterns import FilePattern\n",
    "pattern = FilePattern(make_full_path, time_concat_dim, fsspec_open_kwargs={'anon':True})\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By inspecting our instantiated pattern we see that our pattern has indexed our two files chronologically according to the concatenation key we provided it, and assigned the correct url to each file using the file path function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, fname in pattern.items():\n",
    "    print(index, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time to make the recipe!**\n",
    "- In it's most basic form, `XarrayZarrRecipe` can be instantiated using a file pattern as the only argument. Here we'll be using some of the optional arguments to specify a few additional preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.recipes.xarray_zarr import XarrayZarrRecipe\n",
    "\n",
    "recipe = XarrayZarrRecipe(\n",
    "    pattern, \n",
    "    target_chunks=target_chunks,\n",
    "    process_chunk=set_bnds_as_coords,\n",
    "    xarray_concat_kwargs={'join':'exact'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create storage targets\n",
    "- Here we are caching the netcdf files locally\n",
    "- We also need a temporary metadata cache because we don't know in advance how many time slices are in each netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "from pangeo_forge_recipes.storage import FSSpecTarget, CacheFSSpecTarget, MetadataTarget\n",
    "\n",
    "fs_local = LocalFileSystem()\n",
    "\n",
    "target_dir = tempfile.TemporaryDirectory()\n",
    "target = FSSpecTarget(fs_local, target_dir.name)\n",
    "\n",
    "cache_dir = tempfile.TemporaryDirectory()\n",
    "cache_target = CacheFSSpecTarget(fs_local, cache_dir.name)\n",
    "\n",
    "meta_dir = tempfile.TemporaryDirectory()\n",
    "meta_store = MetadataTarget(fs_local, meta_dir.name)\n",
    "\n",
    "recipe.target = target\n",
    "recipe.input_cache = cache_target\n",
    "recipe.metadata_cache = meta_store\n",
    "\n",
    "cache_target.root_path, target.root_path, meta_store.root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute the recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "for input_key in recipe.iter_inputs():\n",
    "    recipe.cache_input(input_key)\n",
    "\n",
    "# use recipe to create the zarr store:\n",
    "recipe.prepare_target() \n",
    "\n",
    "# is it there?\n",
    "zgroup = zarr.open(target_dir.name)\n",
    "print(zgroup.tree())\n",
    "\n",
    "for chunk in recipe.iter_chunks():\n",
    "    recipe.store_chunk(chunk)\n",
    "recipe.finalize_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Check the resulting Zarr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if it worked:\n",
    "ds = xr.open_zarr(target_dir.name)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[var][-1].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postscript\n",
    "- If you find a CMIP6 dataset for which this recipe does not work, Please report it at [issue#105](https://github.com/pangeo-forge/pangeo-forge-recipes/issues/105) so we can refine the recipe, if possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubles found:\n",
    "\n",
    "dataset = 'IPSL-CM6A-LR.abrupt-4xCO2.r1i1p1f1.Lmon.cLeaf.gr'  # need decode_coords=False in xr.open_dataset, but using xarray_open_kwargs = {'decode_coords':False}, still throws an error when caching the input "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
