Metadata-Version: 2.1
Name: adapter-transformers
Version: 2.3.0
Summary: A friendly fork of Huggingface's Transformers, adding Adapters to PyTorch language models
Home-page: https://github.com/adapter-hub/adapter-transformers
Author: Jonas Pfeiffer, Andreas RÃ¼cklÃ©, Clifton Poth, Hannah Sterz, based on work by Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors
Author-email: pfeiffer@ukp.tu-darmstadt.de
License: Apache
Keywords: NLP deep learning transformer pytorch BERT adapters
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.6.0
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: filelock
Requires-Dist: huggingface-hub (<1.0,>=0.1.0)
Requires-Dist: numpy (>=1.17)
Requires-Dist: packaging (>=20.0)
Requires-Dist: pyyaml (>=5.1)
Requires-Dist: regex (!=2019.12.17)
Requires-Dist: requests
Requires-Dist: sacremoses
Requires-Dist: tokenizers (<0.11,>=0.10.1)
Requires-Dist: tqdm (>=4.27)
Requires-Dist: dataclasses ; python_version < "3.7"
Requires-Dist: importlib-metadata ; python_version < "3.8"
Provides-Extra: all
Requires-Dist: tensorflow (>=2.3) ; extra == 'all'
Requires-Dist: onnxconverter-common ; extra == 'all'
Requires-Dist: keras2onnx ; extra == 'all'
Requires-Dist: torch (>=1.0) ; extra == 'all'
Requires-Dist: sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'all'
Requires-Dist: protobuf ; extra == 'all'
Requires-Dist: tokenizers (<0.11,>=0.10.1) ; extra == 'all'
Requires-Dist: torchaudio ; extra == 'all'
Requires-Dist: librosa ; extra == 'all'
Requires-Dist: Pillow ; extra == 'all'
Requires-Dist: optuna ; extra == 'all'
Requires-Dist: ray[tune] ; extra == 'all'
Requires-Dist: sigopt ; extra == 'all'
Requires-Dist: timm ; extra == 'all'
Requires-Dist: codecarbon (==1.2.0) ; extra == 'all'
Provides-Extra: audio
Requires-Dist: librosa ; extra == 'audio'
Provides-Extra: codecarbon
Requires-Dist: codecarbon (==1.2.0) ; extra == 'codecarbon'
Provides-Extra: deepspeed
Requires-Dist: deepspeed (>=0.5.3) ; extra == 'deepspeed'
Provides-Extra: dev
Requires-Dist: tensorflow (>=2.3) ; extra == 'dev'
Requires-Dist: onnxconverter-common ; extra == 'dev'
Requires-Dist: keras2onnx ; extra == 'dev'
Requires-Dist: torch (>=1.0) ; extra == 'dev'
Requires-Dist: sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'dev'
Requires-Dist: protobuf ; extra == 'dev'
Requires-Dist: tokenizers (<0.11,>=0.10.1) ; extra == 'dev'
Requires-Dist: torchaudio ; extra == 'dev'
Requires-Dist: librosa ; extra == 'dev'
Requires-Dist: Pillow ; extra == 'dev'
Requires-Dist: optuna ; extra == 'dev'
Requires-Dist: ray[tune] ; extra == 'dev'
Requires-Dist: sigopt ; extra == 'dev'
Requires-Dist: timm ; extra == 'dev'
Requires-Dist: codecarbon (==1.2.0) ; extra == 'dev'
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: pytest-xdist ; extra == 'dev'
Requires-Dist: timeout-decorator ; extra == 'dev'
Requires-Dist: parameterized ; extra == 'dev'
Requires-Dist: psutil ; extra == 'dev'
Requires-Dist: datasets ; extra == 'dev'
Requires-Dist: pytest-timeout ; extra == 'dev'
Requires-Dist: black (==21.4b0) ; extra == 'dev'
Requires-Dist: sacrebleu (<2.0.0,>=1.4.12) ; extra == 'dev'
Requires-Dist: rouge-score ; extra == 'dev'
Requires-Dist: nltk ; extra == 'dev'
Requires-Dist: GitPython (<3.1.19) ; extra == 'dev'
Requires-Dist: cookiecutter (==1.7.2) ; extra == 'dev'
Requires-Dist: isort (>=5.5.4) ; extra == 'dev'
Requires-Dist: flake8 (>=3.8.3) ; extra == 'dev'
Requires-Dist: fugashi (>=1.0) ; extra == 'dev'
Requires-Dist: ipadic (<2.0,>=1.0.0) ; extra == 'dev'
Requires-Dist: unidic-lite (>=1.0.7) ; extra == 'dev'
Requires-Dist: unidic (>=1.0.2) ; extra == 'dev'
Requires-Dist: docutils (==0.16.0) ; extra == 'dev'
Requires-Dist: recommonmark ; extra == 'dev'
Requires-Dist: sphinx (==3.2.1) ; extra == 'dev'
Requires-Dist: sphinx-markdown-tables ; extra == 'dev'
Requires-Dist: sphinx-rtd-theme (==0.4.3) ; extra == 'dev'
Requires-Dist: sphinx-copybutton ; extra == 'dev'
Requires-Dist: sphinxext-opengraph (==0.4.1) ; extra == 'dev'
Requires-Dist: sphinx-intl ; extra == 'dev'
Requires-Dist: sphinx-multiversion ; extra == 'dev'
Requires-Dist: scikit-learn ; extra == 'dev'
Provides-Extra: docs
Requires-Dist: tensorflow (>=2.3) ; extra == 'docs'
Requires-Dist: onnxconverter-common ; extra == 'docs'
Requires-Dist: keras2onnx ; extra == 'docs'
Requires-Dist: torch (>=1.0) ; extra == 'docs'
Requires-Dist: sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'docs'
Requires-Dist: protobuf ; extra == 'docs'
Requires-Dist: tokenizers (<0.11,>=0.10.1) ; extra == 'docs'
Requires-Dist: torchaudio ; extra == 'docs'
Requires-Dist: librosa ; extra == 'docs'
Requires-Dist: Pillow ; extra == 'docs'
Requires-Dist: optuna ; extra == 'docs'
Requires-Dist: ray[tune] ; extra == 'docs'
Requires-Dist: sigopt ; extra == 'docs'
Requires-Dist: timm ; extra == 'docs'
Requires-Dist: codecarbon (==1.2.0) ; extra == 'docs'
Requires-Dist: docutils (==0.16.0) ; extra == 'docs'
Requires-Dist: recommonmark ; extra == 'docs'
Requires-Dist: sphinx (==3.2.1) ; extra == 'docs'
Requires-Dist: sphinx-markdown-tables ; extra == 'docs'
Requires-Dist: sphinx-rtd-theme (==0.4.3) ; extra == 'docs'
Requires-Dist: sphinx-copybutton ; extra == 'docs'
Requires-Dist: sphinxext-opengraph (==0.4.1) ; extra == 'docs'
Requires-Dist: sphinx-intl ; extra == 'docs'
Requires-Dist: sphinx-multiversion ; extra == 'docs'
Provides-Extra: docs_specific
Requires-Dist: docutils (==0.16.0) ; extra == 'docs_specific'
Requires-Dist: recommonmark ; extra == 'docs_specific'
Requires-Dist: sphinx (==3.2.1) ; extra == 'docs_specific'
Requires-Dist: sphinx-markdown-tables ; extra == 'docs_specific'
Requires-Dist: sphinx-rtd-theme (==0.4.3) ; extra == 'docs_specific'
Requires-Dist: sphinx-copybutton ; extra == 'docs_specific'
Requires-Dist: sphinxext-opengraph (==0.4.1) ; extra == 'docs_specific'
Requires-Dist: sphinx-intl ; extra == 'docs_specific'
Requires-Dist: sphinx-multiversion ; extra == 'docs_specific'
Provides-Extra: fairscale
Requires-Dist: fairscale (>0.3) ; extra == 'fairscale'
Provides-Extra: flax
Provides-Extra: flax-speech
Requires-Dist: librosa ; extra == 'flax-speech'
Provides-Extra: integrations
Requires-Dist: optuna ; extra == 'integrations'
Requires-Dist: ray[tune] ; extra == 'integrations'
Requires-Dist: sigopt ; extra == 'integrations'
Provides-Extra: ja
Requires-Dist: fugashi (>=1.0) ; extra == 'ja'
Requires-Dist: ipadic (<2.0,>=1.0.0) ; extra == 'ja'
Requires-Dist: unidic-lite (>=1.0.7) ; extra == 'ja'
Requires-Dist: unidic (>=1.0.2) ; extra == 'ja'
Provides-Extra: modelcreation
Requires-Dist: cookiecutter (==1.7.2) ; extra == 'modelcreation'
Provides-Extra: onnx
Requires-Dist: onnxconverter-common ; extra == 'onnx'
Requires-Dist: keras2onnx ; extra == 'onnx'
Requires-Dist: onnxruntime (>=1.4.0) ; extra == 'onnx'
Requires-Dist: onnxruntime-tools (>=1.4.2) ; extra == 'onnx'
Provides-Extra: onnxruntime
Requires-Dist: onnxruntime (>=1.4.0) ; extra == 'onnxruntime'
Requires-Dist: onnxruntime-tools (>=1.4.2) ; extra == 'onnxruntime'
Provides-Extra: optuna
Requires-Dist: optuna ; extra == 'optuna'
Provides-Extra: quality
Requires-Dist: black (==21.4b0) ; extra == 'quality'
Requires-Dist: isort (>=5.5.4) ; extra == 'quality'
Requires-Dist: flake8 (>=3.8.3) ; extra == 'quality'
Provides-Extra: ray
Requires-Dist: ray[tune] ; extra == 'ray'
Provides-Extra: retrieval
Requires-Dist: datasets ; extra == 'retrieval'
Provides-Extra: sagemaker
Requires-Dist: sagemaker (>=2.31.0) ; extra == 'sagemaker'
Provides-Extra: sentencepiece
Requires-Dist: sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'sentencepiece'
Requires-Dist: protobuf ; extra == 'sentencepiece'
Provides-Extra: serving
Requires-Dist: pydantic ; extra == 'serving'
Requires-Dist: uvicorn ; extra == 'serving'
Requires-Dist: fastapi ; extra == 'serving'
Requires-Dist: starlette ; extra == 'serving'
Provides-Extra: sigopt
Requires-Dist: sigopt ; extra == 'sigopt'
Provides-Extra: sklearn
Requires-Dist: scikit-learn ; extra == 'sklearn'
Provides-Extra: speech
Requires-Dist: torchaudio ; extra == 'speech'
Requires-Dist: librosa ; extra == 'speech'
Provides-Extra: testing
Requires-Dist: pytest ; extra == 'testing'
Requires-Dist: pytest-xdist ; extra == 'testing'
Requires-Dist: timeout-decorator ; extra == 'testing'
Requires-Dist: parameterized ; extra == 'testing'
Requires-Dist: psutil ; extra == 'testing'
Requires-Dist: datasets ; extra == 'testing'
Requires-Dist: pytest-timeout ; extra == 'testing'
Requires-Dist: black (==21.4b0) ; extra == 'testing'
Requires-Dist: sacrebleu (<2.0.0,>=1.4.12) ; extra == 'testing'
Requires-Dist: rouge-score ; extra == 'testing'
Requires-Dist: nltk ; extra == 'testing'
Requires-Dist: GitPython (<3.1.19) ; extra == 'testing'
Requires-Dist: cookiecutter (==1.7.2) ; extra == 'testing'
Provides-Extra: tf
Requires-Dist: tensorflow (>=2.3) ; extra == 'tf'
Requires-Dist: onnxconverter-common ; extra == 'tf'
Requires-Dist: keras2onnx ; extra == 'tf'
Provides-Extra: tf-cpu
Requires-Dist: tensorflow-cpu (>=2.3) ; extra == 'tf-cpu'
Requires-Dist: onnxconverter-common ; extra == 'tf-cpu'
Requires-Dist: keras2onnx ; extra == 'tf-cpu'
Provides-Extra: tf-speech
Requires-Dist: librosa ; extra == 'tf-speech'
Provides-Extra: timm
Requires-Dist: timm ; extra == 'timm'
Provides-Extra: tokenizers
Requires-Dist: tokenizers (<0.11,>=0.10.1) ; extra == 'tokenizers'
Provides-Extra: torch
Requires-Dist: torch (>=1.0) ; extra == 'torch'
Provides-Extra: torch-speech
Requires-Dist: torchaudio ; extra == 'torch-speech'
Requires-Dist: librosa ; extra == 'torch-speech'
Provides-Extra: torchhub
Requires-Dist: filelock ; extra == 'torchhub'
Requires-Dist: huggingface-hub (<1.0,>=0.1.0) ; extra == 'torchhub'
Requires-Dist: importlib-metadata ; extra == 'torchhub'
Requires-Dist: numpy (>=1.17) ; extra == 'torchhub'
Requires-Dist: packaging (>=20.0) ; extra == 'torchhub'
Requires-Dist: protobuf ; extra == 'torchhub'
Requires-Dist: regex (!=2019.12.17) ; extra == 'torchhub'
Requires-Dist: requests ; extra == 'torchhub'
Requires-Dist: sacremoses ; extra == 'torchhub'
Requires-Dist: sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'torchhub'
Requires-Dist: torch (>=1.0) ; extra == 'torchhub'
Requires-Dist: tokenizers (<0.11,>=0.10.1) ; extra == 'torchhub'
Requires-Dist: tqdm (>=4.27) ; extra == 'torchhub'
Provides-Extra: vision
Requires-Dist: Pillow ; extra == 'vision'

<!---
Copyright 2020 The AdapterHub Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align="center">
<img style="vertical-align:middle" src="https://raw.githubusercontent.com/Adapter-Hub/adapter-transformers/master/adapter_docs/logo.png" />
</p>
<h1 align="center">
<span>adapter-transformers</span>
</h1>

<h3 align="center">
A friendly fork of HuggingFace's <i>Transformers</i>, adding Adapters to PyTorch language models
</h3>

![Tests](https://github.com/Adapter-Hub/adapter-transformers/workflows/Tests/badge.svg)
[![GitHub](https://img.shields.io/github/license/adapter-hub/adapter-transformers.svg?color=blue)](https://github.com/adapter-hub/adapter-transformers/blob/master/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/adapter-transformers)](https://pypi.org/project/adapter-transformers/)

`adapter-transformers` is an extension of [HuggingFace's Transformers](https://github.com/huggingface/transformers) library, integrating adapters into state-of-the-art language models by incorporating **[AdapterHub](https://adapterhub.ml)**, a central repository for pre-trained adapter modules.

_ðŸ’¡ Important: This library can be used as a drop-in replacement for HuggingFace Transformers and regularly synchronizes new upstream changes.
Thus, most files in this repository are direct copies from the HuggingFace Transformers source, modified only with changes required for the adapter implementations._

## Installation

`adapter-transformers` currently supports **Python 3.6+** and **PyTorch 1.3.1+**.
After [installing PyTorch](https://pytorch.org/get-started/locally/), you can install `adapter-transformers` from PyPI ...

```
pip install -U adapter-transformers
```

... or from source by cloning the repository:

```
git clone https://github.com/adapter-hub/adapter-transformers.git
cd adapter-transformers
pip install .
```

## Getting Started

HuggingFace's great documentation on getting started with _Transformers_ can be found [here](https://huggingface.co/transformers/index.html). `adapter-transformers` is fully compatible with _Transformers_.

To get started with adapters, refer to these locations:

- **[Colab notebook tutorials](https://github.com/Adapter-Hub/adapter-transformers/tree/master/notebooks)**, a series notebooks providing an introduction to all the main concepts of (adapter-)transformers and AdapterHub
- **https://docs.adapterhub.ml**, our documentation on training and using adapters with _adapter-transformers_
- **https://adapterhub.ml** to explore available pre-trained adapter modules and share your own adapters
- **[Examples folder](https://github.com/Adapter-Hub/adapter-transformers/tree/master/examples)** of this repository containing HuggingFace's example training scripts, many adapted for training adapters


## Citation

If you use this library for your work, please consider citing our paper [AdapterHub: A Framework for Adapting Transformers](https://arxiv.org/abs/2007.07779):

```
@inproceedings{pfeiffer2020AdapterHub,
    title={AdapterHub: A Framework for Adapting Transformers},
    author={Pfeiffer, Jonas and
            R{\"u}ckl{\'e}, Andreas and
            Poth, Clifton and
            Kamath, Aishwarya and
            Vuli{\'c}, Ivan and
            Ruder, Sebastian and
            Cho, Kyunghyun and
            Gurevych, Iryna},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
    pages={46--54},
    year={2020}
}
```


